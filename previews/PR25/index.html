<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · TensorTrains.jl</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.png" alt="TensorTrains.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>TensorTrains.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#What-is-a-Tensor-Train?"><span>What is a Tensor Train?</span></a></li><li><a class="tocitem" href="#Notation-and-terminology"><span>Notation and terminology</span></a></li><li><a class="tocitem" href="#Efficient-computation"><span>Efficient computation</span></a></li><li><a class="tocitem" href="#What-can-this-package-do?"><span>What can this package do?</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="guide/">Guide</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/stecrotti/TensorTrains.jl/blob/main/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="What-is-a-Tensor-Train?"><a class="docs-heading-anchor" href="#What-is-a-Tensor-Train?">What is a Tensor Train?</a><a id="What-is-a-Tensor-Train?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-Tensor-Train?" title="Permalink"></a></h2><p>A <a href="https://tensornetwork.org/mps/">Tensor Train</a> is a type of tensor factorization involving the product of 3-index tensors organized on a one-dimensional chain.  In the context of function approximation and probability, a function of <span>$L$</span> discrete variables is in Tensor Train format if it is written as</p><p class="math-container">\[f(x^1, x^2, \ldots, x^L) = \sum_{a^1,a^2,\ldots,a^{L-1}} [A^1(x^1)]_{a^1}[A^2(x^2)]_{a^1,a^2}\cdots [A^{L-1}(x^{L-1})]_{a^{L-2},a^{L-1}}[A^L(x^L)]_{a^{L-1}}\]</p><p>where, for every choice of <span>$x^l$</span>, <span>$A^l(x^l)$</span> is a real-valued matrix and the matrix sizes must be compatible. The first matrix must have 1 row and the last matrix should have 1 column, such that the product correctly returns a scalar.</p><p>The Tensor Train factorization can be used to parametrize probability distributions, which is the main focus of this package. In this case, <span>$f$</span> should be properly normalized and always return a non-negative value. </p><h3 id="Tensor-Trains-with-Periodic-Boundary-Conditions"><a class="docs-heading-anchor" href="#Tensor-Trains-with-Periodic-Boundary-Conditions">Tensor Trains with Periodic Boundary Conditions</a><a id="Tensor-Trains-with-Periodic-Boundary-Conditions-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-Trains-with-Periodic-Boundary-Conditions" title="Permalink"></a></h3><p>A slight generalization, useful to describe systems with periodic boundary conditions is the following:</p><p class="math-container">\[f(x^1, x^2, \ldots, x^L) = \sum_{a^1,a^2,\ldots,a^{L}} [A^1(x^1)]_{a^1,a^2}[A^2(x^2)]_{a^2,a^3}\cdots [A^{L-1}(x^{L-1})]_{a^{L-1},a^{L}}[A^L(x^L)]_{a^{L},a^1}\]</p><p>In other words, to evaluate <span>$f$</span> one takes the trace of the product of matrices.</p><h2 id="Notation-and-terminology"><a class="docs-heading-anchor" href="#Notation-and-terminology">Notation and terminology</a><a id="Notation-and-terminology-1"></a><a class="docs-heading-anchor-permalink" href="#Notation-and-terminology" title="Permalink"></a></h2><p>Tensor Trains are the most basic type of <a href="https://tensornetwork.org/">Tensor Network</a>. Tensor networks are a large family of tensor factorizations which are often best represented in diagrammatic notation. For this reason, the term <em>bond</em> is used interchangeably as <em>index</em>. The indices <span>$a^1,a^2,\ldots,a^{L-1}$</span> are usually called the <em>virtual indices</em>, while <span>$x^1, x^2, \ldots, x^L$</span> are the <em>physical indices</em>.</p><p>Tensor Trains are used to parametrize wavefunctions in many-body quantum physics. The resulting quantum state is called <a href="https://en.wikipedia.org/wiki/Matrix_product_state">Matrix Product State</a>. In such context, the entries are generally complex numbers, and a probability can be obtained for a given state by taking the squared absolute value of the wavefunction.</p><p>In this package we focus on the &quot;classical&quot; case where the Tensor Train directly represents a probability distribution <span>$p(x^1, x^2, \ldots, x^L)$</span>. </p><h2 id="Efficient-computation"><a class="docs-heading-anchor" href="#Efficient-computation">Efficient computation</a><a id="Efficient-computation-1"></a><a class="docs-heading-anchor-permalink" href="#Efficient-computation" title="Permalink"></a></h2><p>Given a Tensor Train some simple recursive strategies can be employed to</p><h3 id="Compute-the-normalization"><a class="docs-heading-anchor" href="#Compute-the-normalization">Compute the normalization</a><a id="Compute-the-normalization-1"></a><a class="docs-heading-anchor-permalink" href="#Compute-the-normalization" title="Permalink"></a></h3><p class="math-container">\[Z = \sum_{x^1, x^2, \ldots, x^L} \sum_{a^1,a^2,\ldots,a^{L-1}} [A^1(x^1)]_{a^1}[A^2(x^2)]_{a^1,a^2}\cdots [A^{L-1}(x^{L-1})]_{a^{L-2},a^{L-1}}[A^L(x^L)]_{a^{L-1}}\]</p><p>such that </p><p class="math-container">\[\begin{aligned}
1&amp;=\sum_{x^1, x^2, \ldots, x^L}p(x^1, x^2, \ldots, x^L)\\&amp;=\sum_{x^1, x^2, \ldots, x^L}\frac1Z \sum_{a^1,a^2,\ldots,a^{L-1}} [A^1(x^1)]_{a^1}[A^2(x^2)]_{a^1,a^2}\cdots [A^{L-1}(x^{L-1})]_{a^{L-2},a^{L-1}}[A^L(x^L)]_{a^{L-1}}
\end{aligned}\]</p><h3 id="Compute-marginals"><a class="docs-heading-anchor" href="#Compute-marginals">Compute marginals</a><a id="Compute-marginals-1"></a><a class="docs-heading-anchor-permalink" href="#Compute-marginals" title="Permalink"></a></h3><p>Single-variable</p><p class="math-container">\[p(x^l=x) = \sum_{x^1, x^2, \ldots, x^L} p(x^1, x^2, \ldots, x^L) \delta(x^l,x)\]</p><p>and two-variable</p><p class="math-container">\[p(x^l=x, x^m=x&#39;) = \sum_{x^1, x^2, \ldots, x^L} p(x^1, x^2, \ldots, x^L) \delta(x^l,x)\delta(x^m,x&#39;)\]</p><h3 id="Extract-exact-samples"><a class="docs-heading-anchor" href="#Extract-exact-samples">Extract exact samples</a><a id="Extract-exact-samples-1"></a><a class="docs-heading-anchor-permalink" href="#Extract-exact-samples" title="Permalink"></a></h3><p>Via hierarchical sampling</p><p class="math-container">\[p(x^1, x^2, \ldots, x^L) = p(x^1)p(x^2|x^1)p(x^3|x^1,x^2)\cdots p(x^L|x^1,x^2,\ldots,x^{L-1})\]</p><p>by first sampling <span>$x^1\sim p(x^1)$</span>, then <span>$x^2\sim p(x^2|x^1)$</span> and so on.</p><h2 id="What-can-this-package-do?"><a class="docs-heading-anchor" href="#What-can-this-package-do?">What can this package do?</a><a id="What-can-this-package-do?-1"></a><a class="docs-heading-anchor-permalink" href="#What-can-this-package-do?" title="Permalink"></a></h2><p>This small package provides some utilities for creating, manipulating and evaluating Tensor Trains interpreted as functions, with a focus on the probabilistic side.  Each variable <span>$x^l$</span> is assumed to be multivariate. Whenever performing some probability-related operation, it is responsability of the user to make sure that the Tensor Train always represents a valid probability distribution.</p><p>Common operations are:</p><ul><li><code>evaluate</code> a Tensor Train at a given set of indices</li><li><code>orthogonalize_left!</code>, <code>orthogonalize_right!</code>: bring a Tensor Train to <a href="https://tensornetwork.org/mps/">left/right orthogonal form</a></li><li><code>compress!</code> a Tensor Train using <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a>-based truncations</li><li><code>normalize!</code> a Tensor Train in the probability sense (not in the <span>$L_2$</span> norm sense!), see above</li><li><code>sample</code> from a Tensor Train intended as a probability ditribution, see above</li><li><code>+</code>,<code>-</code>: take the sum/difference of two TensorTrains</li></ul><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>Let&#39;s construct and initialize at random a Tensor Train of the form</p><p class="math-container">\[f\left((x^1,y^1), (x^2,y^2), (x^3,y^3)\right) = \sum_{a^1,a^2} [A^1(x^1,y^1)]_{a^1}[A^2(x^2,y^2)]_{a^1,a^2}[A^3(x^3,y^3)]_{a^2}\]</p><p>where <span>$x^l\in\{1,2\}, y^l\in\{1,2,3\}$</span>.</p><pre><code class="language-julia hljs">using TensorTrains
L = 3        # length
q = (2, 3)   # number of values taken by x, y
d = 5        # bond dimension
A = rand_tt(d, L, q...)    # construct Tensor Train with random positive entries
xy = [[rand(1:qi) for qi in q] for _ in 1:L]    # random set of indices
p = evaluate(A, xy)    # evaluate `A` at `xy`
compress!(A; svd_trunc=TruncThresh(1e-8));    # compress `A` to reduce the bond dimension
pnew = evaluate(A, xy)
ε = abs((p-pnew)/p)</code></pre><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ul><li>https://tensornetwork.org: &quot;an open-source review article focused on tensor network algorithms, applications, and software&quot;</li><li>Oseledets, I.V., 2011. <a href="https://sites.pitt.edu/~sjh95/related_papers/tensor_train_decomposition.pdf">Tensor-train decomposition</a>. SIAM Journal on Scientific Computing, 33(5).</li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="guide/">Guide »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 18 December 2023 13:28">Monday 18 December 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
